# Near_Real_Time_Data_Warehouse_Analysis
A near-real-time Data Warehouse prototype for a Shopping Store.
1. Assessment task
The student has to design, implement, and analyse a near-real-time Data Warehouse (DW)
prototype for METRO shopping store in Pakistan.
2. Project overview
METRO is one of the biggest superstores chains in Pakistan. The stores have thousands of
customers and therefore it is important for the store to online analyse the shopping behaviour of
their customers. Based on that the store can optimise their selling strategies e.g., giving
promotions on different products.
![image](https://github.com/user-attachments/assets/7b83f2e3-8c4a-471f-bb9c-f0d737b8ea5a)

Figure 1: An overview of METRO DW

Now, to make this analysis of shopping behaviour practical there is a need of building a near-real-
time DW and customers’ transactions from Data Sources (DSs) are required to reflect into DW as

soon as they appear in DSs. The overview of METRO DW is presented in Figure 1. To build a near-
real-time DW we need to implement a near-real-time ETL (Extraction, Transformation, and

Loading) tools. Since the data generated by customers is incomplete as it required by DW, it needs
to complete in the transformation layer of ETL. For example, enriching some information from
Master Data (MD) as shown in Figure 2.

![image](https://github.com/user-attachments/assets/706275f9-d6cf-4d58-8481-3be63ee0d7e2)

To implement this enrichment feature in the transformation phase of ETL we need a Stream-
Relation join operator. There are a number of algorithms available to implement this join

operation however, the seminal one is MESHJOIN (Mesh Join) which is explained in next section
and you will implement its extended version in this project using Java with Eclipse IDE.
3. MESHJOIN (Mesh Join)
The MESHJOIN (Mesh Join) algorithm has been introduced by Polyzotis in 2008 with objective of
implementing the Stream- Relation join operation in the transformation phase of ETL.
The main components of MESHJOIN are: The disk-buffer which will be an array and used to load
the disk partitions in memory. Typically, MD is large, it has to be loaded in memory in partitions.
Normally, the size of each partition in MD is equal to the size of the disk-buffer. Also, MD is
traversed cyclically in an endless loop. The hash table which stores the customers’ transactions
(tuples). The queue is used to keep the record of all the customers’ transactions in memory with
respect to their arrival times. The queue has same number of partitions as MD to make sure that
each tuple has joined with the whole MD before leaving the join operator. The stream-buffer will
be an array and is used to hold the customer transaction meanwhile the algorithm completes one
iteration.
![image](https://github.com/user-attachments/assets/befb7e3e-6f98-466b-89cc-853ac3df5298)

Figure 3: Working of MESHJOIN when R2 is in memory but not yet processed

The crux of MESHJOIN is that with every loop step a new chunk of customers’ transactions is read
into main memory (Hash table) and MD partition in the disk-buffer is replaced by the new MD
partition from the disk. Each of these chunks will remain in main memory for the time of one full
MD cycle. The chunks therefore leave main memory in the order that they enter main memory
and their time of residence in main memory is overlapping. This leads to the staggered processing
pattern of MESHJOIN. In main memory, the incoming customers’ data is organized in a queue,
each chunk being one element of the queue. Figure 3 with four MD partitions shows a pictorial
representation of the MESHJOIN operation: at each point in time, each chunk Si in the queue has
seen a larger number of partitions than the previous, and started at a later position in MD (except

for the case that the traversal of MD resets to the start of MD). The figure shows the moment
when partition R2 of MD is read into the disk-buffer but is not yet processed.
After loading the disk partition into the disk buffer, the algorithm probes each tuple of the disk
buffer in the hash table. If a matching tuple is found, the algorithm generates the join output.
After each iteration the algorithm removes the oldest chunk of customers’ transactions from the
hash table along with their pointers from the queue. This chunk is found at the end of the queue;
its tuples were joined with the whole of MD and are thus completely processed now.
4. Star-schema
The star schema (which you will use in this project) is a data modelling technique that is used to
map multidimensional decision support data into a relational database. Star-schema yields an
easily implemented model for multidimensional data analysis while still preserving the relational
structures on which the operational database is built.
The star schema represents aggregated data for specific business activities. Using the schema, one
can create multiple aggregated data sources that will represent different aspects of business
operations. For example, the aggregation may involve total sales by selected time periods, by
products, by stores, and so on. Aggregated totals can be total product sold, total sales values by
products, etc. The basic star schema has three main components: facts, dimensions, attributes, and
classification levels. Figure 2 can help you to determine the right components for your star
schema.
5. Implementation of Extended MESHJOIN
To implement MESHJOIN algorithm you will implement the following steps using Java Eclipse.
1. Read a segment of stream from TRANSACTIONS table as an input data into the hash table
with their join attribute values in the queue.
2. Load next MD partitions form the both MD tables (customer and product) into the disk
buffers. After the last partition the next partition to read will be the first partition in each
table.
3. Perform join operation between the MD tuples and stream tuples.
4. If the join is successful, enrich the required information to the transaction tuple. It is
important to note that the attribute TOTAL_SALE does not exist in both TRANSACTION and
MD.
5. The transaction tuples will then be loaded into DW. Make sure the dimension tables will
not have the duplication records.
6. Repeat steps 1 to 5 until you load all the data from TRANSACTIONS table to DW.
6. DW analysis
7. 
Once the entire data has been loaded into DW, you will be required to analyse your DW by
applying following OLAP queries.

Q1. Top Revenue-Generating Products on Weekdays and Weekends with Monthly Drill-Down
Find the top 5 products that generated the highest revenue, separated by weekday and weekend
sales, with results grouped by month for a specified year.

Q2. Trend Analysis of Store Revenue Growth Rate Quarterly for 2017
Calculate the revenue growth rate for each store on a quarterly basis for 2017.
Q3. Detailed Supplier Sales Contribution by Store and Product Name
For each store, show the total sales contribution of each supplier broken down by product name. The
output should group results by store, then supplier, and then product name under each supplier.

Q4. Seasonal Analysis of Product Sales Using Dynamic Drill-Down
Present total sales for each product, drilled down by seasonal periods (Spring, Summer, Fall,
Winter). This can help understand product performance across seasonal periods.

Q5. Store-Wise and Supplier-Wise Monthly Revenue Volatility
Calculate the month-to-month revenue volatility for each store and supplier pair. Volatility can be
defined as the percentage change in revenue from one month to the next, helping identify stores
or suppliers with highly fluctuating sales.

Q6. Top 5 Products Purchased Together Across Multiple Orders (Product Affinity Analysis)
Identify the top 5 products frequently bought together within a set of orders (i.e., multiple
products purchased in the same transaction). This product affinity analysis could inform potential
product bundling strategies.

Q7. Yearly Revenue Trends by Store, Supplier, and Product with ROLLUP
Use the ROLLUP operation to aggregate yearly revenue data by store, supplier, and product,
enabling a comprehensive overview from individual product-level details up to total revenue per
store. This query should provide an overview of cumulative and hierarchical sales figures.

Q8. Revenue and Volume-Based Sales Analysis for Each Product for H1 and H2
For each product, calculate the total revenue and quantity sold in the first and second halves of
the year, along with yearly totals. This split-by-time-period analysis can reveal changes in product
popularity or demand over the year.

Q9. Identify High Revenue Spikes in Product Sales and Highlight Outliers
Calculate daily average sales for each product and flag days where the sales exceed twice the daily
average by product as potential outliers or spikes. Explain any identified anomalies in the report,
as these may indicate unusual demand events.

Q10. Create a View STORE_QUARTERLY_SALES for Optimized Sales Analysis
Create a view named STORE_QUARTERLY_SALES that aggregates total quarterly sales by store,
ordered by store name. This view allows quick retrieval of store-specific trends across quarters,
significantly improving query performance for regular sales analysis.

7. Tasks break-up
Following is list of tasks that you need to complete in this project.
1. Identifying appropriate dimension tables, fact table, and their attributes for the sales
scenario presented in Figure 2. Based on that create a star-schema for DW with
appropriate primary and foreign keys.
2. Implementing the MESHJOIN algorithm (using steps described in Section 6) in Java for
successfully loading transactional data into DW after joining it with MD.

3. Applying of different analysis (described in Section 7) on DW using slicing, dicing, drill
down, and materialising view concepts.
4. Writing a project report that should include project overview, schema for DW, MESHJOIN
algorithm, any three shortcomings in Mesh Join, and what did you learn from the project?
